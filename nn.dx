' (Start)

' Create some data

grid: ((Fin 10) & (Fin 10)) => (Float & Float)  =
   for (i, j).
       (IToF (ordinal i),
        IToF (ordinal j))

x = grid
y = for i.
   select ((fst x.i) > 3.0) 1 0 

def to_vec ((a, b):(Float & Float)) : (Fin 2) => Float =
    [a, b]

:plot grid


' Neural network

def relu (input : Float) : Float =
  select (input > 0.0) input 0.0

def linear_layer (input : h => Float)
                 (weight: h => h2 => Float)
                 (bias: h2 => Float) : h2 => Float =
    for i : h2.
       bias.i + sum for j : h. weight.j.i * input.j
    

' Initialization

hidden = 10
key1 = (newKey 0)
key2 = (newKey 1)
(k11, k12) = splitKey key1
(k21, k22) = splitKey key2

' Weights and biases

w1 : ((Fin 2) => (Fin 10) => Float) = for i j. rand (ixkey k11 i)
b1 : ((Fin 10) => Float) = for i. rand (ixkey k12 i)

w2 : ((Fin 10) => (Fin 2) => Float) = for i j. rand (ixkey k21 i)
b2 : ((Fin 2) => Float) = for i. rand (ixkey k22 i)

params = (w1, b1, w2, b2)

:t params

  
' Type of the parameters

def Parameters (h : Type) (h2 : Type) (class: Type) : Type =
  ((h => h2 => Float) &
   (h2 => Float) &
   (h2 => class => Float) &
   (class => Float))

def forward
    ((w1, b1, w2, b2) : Parameters h h2 class) 
    (input : (h => Float))
    (target: Int32)
      : Float = 
  out1 = linear_layer input w1 b1
  out2 = for j. relu out1.j
  out3 = linear_layer out2 w2 b2
  (logsoftmax out3).(target@class)

def loss (params : Parameters (Fin 2) (Fin 10) (Fin 2)) : Float =
  -mean for i. forward params (to_vec x.i) y.i 

grad_forward = grad loss

' SGD

rate = 0.003

loop = withState params \ paramsRef.
  for i : (Fin 100).
      loss : Float  = loss (get paramsRef)
      grads : Parameters (Fin 2) (Fin 10) (Fin 2)  = grad_forward (get paramsRef)
      (dw1, db1, dw2, db2) = grads
      params2 = (w1 - rate .* dw1, b1 - rate .* db1,
                 w2 - rate .* dw2, b2 - rate .* db2)
      paramsRef := params2
      loss

epochloss = (fst loop)
epoch  = for i : (Fin 100). IToF (ordinal i)
:t epochloss

:plot zip epoch epochloss 

:t (snd loop):Parameters (Fin 2) (Fin 10) (Fin 2)



 


-- want to learn the parameters of w1 b1 w2 b2


-- 1 - get gradient of forward
-- 2 - loop
-- 3 - state in order to update
